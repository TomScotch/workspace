
Good sense is the most fairly distributed commodity in the world, Descartes once quipped, because nobody thinks he needs any more of it than he already has. A neat illustration of the fact that gullibility seems to be a disease of other people was provided by Martin Gardner, a great American debunker of pseudoscience, who died this year. In the second edition of his “Fads and Fallacies in the Name of Science” (1957), Gardner reported that most of the irate letters he received in response to the first edition criticised only one of its 26 chapters and found the rest to be fine. Needless to say, readers disagreed about which chapter was the faulty one. Homeopaths objected to the treatment meted out to themselves, but thought that the exposé of chiropractors was spot on, and vice versa.

No group of believers has more reason to be sure of its own good sense than today’s professional scientists. There is, or should be, no mystery about why it is always more rational to believe in science than in anything else, because this is true merely by definition. What makes a method of enquiry count as scientific is not that it employs microscopes, rats, computers or people in stained white coats, but that it seeks to test itself at every turn. If a method is as rigorous and cautious as it can be, it counts as good science; if it isn’t, it doesn’t. Yet this fact sets a puzzle. If science is careful scepticism writ large, shouldn’t a scientific cast of mind require one to be sceptical of science itself?

There is no full-blown logical paradox here. If a claim is ambitious, people should indeed tread warily around it, even if it comes from scientists; it does not follow that they should be sceptical of the scientific method itself. But there is an awkward public-relations challenge for any champion of hard-nosed science. When scientists confront the deniers of evolution, or the devotees of homeopathic medicine, or people who believe that childhood vaccinations cause autism—all of whom are as demonstrably mistaken as anyone can be—they understandably fight shy of revealing just how riddled with error and misleading information the everyday business of science actually is. When you paint yourself as a defender of the truth, it helps to keep quiet about how often you are wrong.

That fact partly explains why some influential climate scientists today, and the UN’s Intergovernmental Panel on Climate Change, are having a hard time. Wary of yielding any ground to those who think that global warming is some sort of hoax, they have sometimes been mightily unwilling to be open about exaggerations, mistakes and confusions in influential reports about climate change—such as the flawed “Hockey Stick” paper, published in Nature in 1998, which estimated global temperatures over the past 600 years, and has become one of the most cited publications on the topic. This defensiveness has backfired, and the credibility of climatologists has suffered.

At the end of her book “Science: A Four Thousand Year History” (2009), Patricia Fara of Cambridge University wrote that “there can be no cast-iron guarantee that the cutting-edge science of today will not represent the discredited alchemy of tomorrow”. This is surely an understatement. If the past is any guide—and what else could be?—plenty of today’s science will be discredited in future. There is no reason to think that today’s practitioners are uniquely immune to the misconceptions, hasty generalisations, fads and hubris that marked most of their predecessors. Although the best ideas of Copernicus, Galileo, Newton, Boyle, Darwin, Einstein and others have stood the test of time and taken their place in the permanent corpus of knowledge, error remains inherent in the enterprise of science. This is because interesting theories always go beyond the data that they seek to explain, and because science is made by people. Examples from recent decades of scientific consensus that turned out to be wrong range from the local to the largest possible scale: acid rain was not destroying forests in Germany in the 1980s, as it was said to have been, and the expansion of the universe has not been slowing down, as cosmologists used to think it was.

Physicists, in particular, have long believed themselves to be on the verge of explaining almost everything. In 1894 Albert Michelson, the first American to get a Nobel prize in science, said that all the main laws and facts of physics had already been discovered. In 1928 Max Born, another Nobel prize-winner, said that physics would be completed in about six months’ time. In 1988, in his bestselling “A Brief History of Time”, the cosmologist Stephen Hawking wrote that “we may now be near the end of the search for the ultimate laws of nature.” Now, in the newly published “The Grand Design”, Hawking paints a picture of the universe that is “different…from the picture we might have painted just a decade or two ago”. In the long run, physicists are, no doubt, getting closer and closer to the truth. But you can never be sure when the long run has arrived. And in the short run—to adapt Keynes’s proverb—we are often all wrong. 

Most laymen probably assume that the 350-year-old institution of “peer review”, which acts as a gatekeeper to publication in scientific journals, involves some attempt to check the articles that see the light of day. In fact they are rarely checked for accuracy, and, as a study for the Fraser Institute, a Canadian think-tank, reported last year, “the data and computational methods are so seldom disclosed that post-publication verification is equally rare.” Journals will usually consider only articles that present positive and striking results, and scientists need constantly to publish in order to keep their careers alive. So it is that, like the late comedian Danny Kaye, professional scientists sometimes get their exercise by jumping to conclusions. Historians of science call this bias the “file-drawer problem”: if a set of experiments produces a result contrary to what the team needs to find, it ends up filed away, and the world never finds out about it.

In a recent book, “Wrong: Why Experts Keep Failing Us—And How to Know When Not to Trust Them”, David Freedman, an American business and science journalist, does a sobering job of reviewing dozens of studies of ignorance, bias, error and outright fraud in recent academic science. He notes that discredited research is regularly cited in support of other research, even after it has been discredited. Trials of the safety and efficacy of drugs, which are often paid for by pharmaceutical companies, seem to be especially liable to errors of various sorts. That helps to explain why medicines that can do unexpected harm—such as Thalidomide, the sedative which was withdrawn in 1961 after causing deformities in babies, and Vioxx, a painkiller that had been used by 84m people before it was pulled in 2004—make it to the market.

It is perhaps the biases of science reporting in the popular press that produce the most misinformation, especially in medicine. The faintest whiff of a breakthrough treatment for a common disease is news, yet the fact that yesterday’s breakthrough didn’t pan out—which ought to be equally interesting to a seeker after truth—rarely is. When a drug is tested on animals and seems promising, it makes headlines, even though the majority of drugs that pass animal trials never become usable for people. And barely a day goes by without the media exploiting an almost universal misunderstanding of statistics and reporting something that has no relevance to anything. When researchers are said to have found that an effect occurs to a statistically significant degree, this means that it probably isn’t caused by a fluke, not that it is large or definite enough to be useful.

A school of ancient philosophers, the followers of Pyrrho of Elis (who died C270BC), came up with a consistent but impractical response to the problem of whom to believe when expert sources disagree or are found to be unreliable. Believe nobody, they said: suspend judgment on everything. Scholars have debated whether anyone could have lived a life according to this principle, and the consensus is no, they could not. Suspending judgment may keep you free from erroneous beliefs, but it also makes it impossible to decide rationally on what to do about anything.

Happily, there is another way out of the impasse between fallible science and even-more-fallible non-science. The contest is not a zero-sum game: the shortcomings of science do not make it rational to believe cranks instead. It’s a fair bet that many of today’s scientific beliefs are wrong, but only your grandchildren will know which ones, and in the meantime, science is the only game in town. Or, as Hilaire Belloc put it, in a rather different context:

...always keep a-hold of Nurse
For fear of finding something worse.
